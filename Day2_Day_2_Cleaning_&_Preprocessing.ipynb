{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e18aed",
   "metadata": {},
   "source": [
    "# Day 2 — Data Acquisition, Cleaning & Preprocessing\n",
    "\n",
    "**Duration:** 2 hours\n",
    "\n",
    "**Objectives:**\n",
    "- Learn common data sources and acquisition methods\n",
    "- Practice identifying and fixing common data problems\n",
    "- Preprocess features for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ddecfd",
   "metadata": {},
   "source": [
    "## 1. Why data cleaning matters\n",
    "\n",
    "Most real data is messy. Expect to spend 60–80% of time cleaning and preparing data. Dirty data leads to poor models (GIGO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41f472",
   "metadata": {},
   "source": [
    "## 2. Data sources & acquisition methods\n",
    "\n",
    "- Files: CSV, Excel\n",
    "- Databases: SQL\n",
    "- APIs: REST, JSON\n",
    "- Web scraping\n",
    "- Sensors / IoT\n",
    "\n",
    "We'll focus on file-based and in-memory examples for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d26620",
   "metadata": {},
   "source": [
    "## 3. Common data issues\n",
    "\n",
    "- Missing values\n",
    "- Duplicates\n",
    "- Wrong datatypes\n",
    "- Outliers\n",
    "- Inconsistent formatting (e.g., '50,000' vs 50000)\n",
    "\n",
    "We will address techniques for handling these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530cb533",
   "metadata": {},
   "source": [
    "## 4. Loading data in pandas (demo)\n",
    "\n",
    "We'll demonstrate loading CSV and previewing data. Here we use the Titanic dataset again as it's convenient and illustrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "# create a copy to experiment on\n",
    "df = titanic.copy()\n",
    "# show first rows and info\n",
    "print(df.head())\n",
    "print('\\nINFO:\\n')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c7d59",
   "metadata": {},
   "source": [
    "## 5. Missing values — detection\n",
    "\n",
    "Look for nulls using `isnull().sum()` and visual inspection. Decide when to drop vs impute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d5b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f3ee0",
   "metadata": {},
   "source": [
    "## 6. Handling missing values — strategies\n",
    "\n",
    "- Drop rows/columns\n",
    "- Impute with mean/median/mode\n",
    "- Forward/backward fill for time-series\n",
    "- Model-based imputation (advanced)\n",
    "\n",
    "We'll impute `age` with median and `embarked` with mode as a simple strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute age with median and embarked with mode (inplace copy)\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "imputer_age = SimpleImputer(strategy='median')\n",
    "df['age'] = imputer_age.fit_transform(df[['age']])\n",
    "\n",
    "imputer_emb = SimpleImputer(strategy='most_frequent')\n",
    "df['embarked'] = imputer_emb.fit_transform(df[['embarked']])\n",
    "\n",
    "print(df[['age','embarked']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176a3e2",
   "metadata": {},
   "source": [
    "## 7. Duplicates\n",
    "\n",
    "Use `drop_duplicates()` to remove exact duplicates. Be careful: near-duplicates may need domain logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate duplicates (Titanic has none usually)\n",
    "df.shape\n",
    "# if duplicates existed we'd use:\n",
    "# df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa53286e",
   "metadata": {},
   "source": [
    "## 8. Outliers — detection & handling\n",
    "\n",
    "Common methods: boxplot visualization, IQR rule, z-score. We'll show IQR method for fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x=df['fare'].dropna())\n",
    "plt.title('Boxplot of Fare')\n",
    "plt.show()\n",
    "\n",
    "# IQR method\n",
    "Q1 = df['fare'].quantile(0.25)\n",
    "Q3 = df['fare'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5*IQR\n",
    "upper = Q3 + 1.5*IQR\n",
    "print('fare range (IQR):', lower, upper)\n",
    "\n",
    "# Count potential outliers\n",
    "outliers = df[(df['fare'] < lower) | (df['fare'] > upper)]\n",
    "print('Potential outliers count:', outliers.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cb2aa",
   "metadata": {},
   "source": [
    "## 9. Categorical encoding\n",
    "\n",
    "Convert categorical variables to numeric: Label Encoding vs One-Hot Encoding. Example: `sex` and `class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0159764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'class' and label encode 'sex'\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_enc = df.copy()\n",
    "le = LabelEncoder()\n",
    "df_enc['sex_encoded'] = le.fit_transform(df_enc['sex'].astype(str))\n",
    "# one-hot for 'class'\n",
    "df_enc = pd.get_dummies(df_enc, columns=['class'], prefix='class')\n",
    "\n",
    "print(df_enc[['sex','sex_encoded']].head())\n",
    "print('\\nOne-hot columns:', [c for c in df_enc.columns if c.startswith('class_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e0a31",
   "metadata": {},
   "source": [
    "## 10. Scaling & Normalization\n",
    "\n",
    "Why scale? Many ML algorithms are sensitive to feature scale (e.g., KNN, SVM). Methods: MinMaxScaler, StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_enc['fare_scaled'] = scaler.fit_transform(df_enc[['fare']].fillna(0))\n",
    "\n",
    "print(df_enc[['fare','fare_scaled']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd05e0f",
   "metadata": {},
   "source": [
    "## 11. Putting it together — simple preprocessing pipeline\n",
    "\n",
    "Steps applied: imputation, encoding, scaling. This prepares data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple feature matrix and label for modeling demo\n",
    "features = ['age','fare_scaled','sex_encoded']\n",
    "# drop rows with missing label\n",
    "df_model = df_enc.dropna(subset=['survived'])\n",
    "X = df_model[features]\n",
    "y = df_model['survived'].astype(int)\n",
    "\n",
    "print('X shape:', X.shape, 'y shape:', y.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d85f22",
   "metadata": {},
   "source": [
    "## 12. Exercise (in-notebook)\n",
    "\n",
    "1. Try a different imputation strategy for `age` (mean instead of median). Compare means before/after.\n",
    "2. Create one additional visualization: survival vs fare (bin fare into groups and plot survival rate).\n",
    "\n",
    "(Optional) Try scaling with StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise starters\n",
    "# 1) Mean imputation\n",
    "import numpy as np\n",
    "\n",
    "df_alt = titanic.copy()\n",
    "df_alt['age'] = df_alt['age'].fillna(df_alt['age'].mean())\n",
    "print('Mean age after imputation:', df_alt['age'].mean())\n",
    "\n",
    "# 2) Survival vs fare bins\n",
    "bins = [0,10,20,50,100,600]\n",
    "labels = ['0-10','10-20','20-50','50-100','100+']\n",
    "df_model['fare_bin'] = pd.cut(df_model['fare'].fillna(0), bins=bins, labels=labels)\n",
    "print(df_model.groupby('fare_bin')['survived'].mean())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "survival_by_fare = df_model.groupby('fare_bin')['survived'].mean().reset_index()\n",
    "sns.barplot(x='fare_bin', y='survived', data=survival_by_fare)\n",
    "plt.title('Survival rate by Fare bin')\n",
    "plt.xlabel('Fare bin')\n",
    "plt.ylabel('Survival rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277b5c1",
   "metadata": {},
   "source": [
    "## 13. Wrap-up & Reading\n",
    "\n",
    "Suggested reading: Wes McKinney (Ch.3-4). Tomorrow: Day 3 — Statistics & Probability."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
